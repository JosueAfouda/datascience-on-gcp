# üß≠ Probl√©matique du Projet

Dans le cadre de ce projet, nous cherchons √† **prendre une d√©cision inform√©e quant √† l'annulation ou non d'un rendez-vous professionnel**, en nous basant sur la **probabilit√© d'arriv√©e √† l'heure d'un vol**. Le contexte est celui d'un voyageur qui doit d√©cider, **avant l'arriv√©e de son vol**, s‚Äôil est pr√©f√©rable de maintenir ou d‚Äôannuler une r√©union planifi√©e √† destination.

La r√®gle de d√©cision adopt√©e est la suivante :
‚û°Ô∏è **Annuler la r√©union si la probabilit√© que le vol arrive √† moins de 15 minutes de retard est inf√©rieure √† 70%.**

Pour mod√©liser cette probabilit√© de ponctualit√©, nous utiliserons des **donn√©es historiques sur les vols** disponibles publiquement via le [Bureau of Transportation Statistics (BTS)](https://www.transtats.bts.gov/) des √âtats-Unis. Ces donn√©es, collect√©es depuis 1987, d√©crivent de mani√®re d√©taill√©e les performances des compagnies a√©riennes en mati√®re de retards √† l‚Äôarriv√©e. Le jeu de donn√©es utilis√©, appel√© [Airline On-Time Performance Data](https://www.bts.gov/topics/airline-time-tables), a √©t√© con√ßu pr√©cis√©ment pour mesurer et surveiller la ponctualit√© des vols.

Ce projet vise donc √† :

* Estimer la **probabilit√© d‚Äôarriv√©e √† l‚Äôheure d‚Äôun vol donn√©** √† partir de ses caract√©ristiques (compagnie, a√©roport, m√©t√©o, heure de d√©part, etc.),
* Et **automatiser la prise de d√©cision** d‚Äôannulation d‚Äôun rendez-vous sur la base de cette estimation probabiliste.

---

# Data Ingestion

---

## ‚úàÔ∏è Description des colonnes du dataset (retards de vols)

### üìÖ Informations temporelles

* **Year** : Ann√©e du vol (ex. 2015)
* **Quarter** : Trimestre (1 √† 4)
* **Month** : Mois (1 √† 12)
* **DayofMonth** : Jour du mois (1 √† 31)
* **DayOfWeek** : Jour de la semaine (1 = Lundi, 7 = Dimanche)
* **FlightDate** : Date compl√®te du vol (`YYYY-MM-DD`)

### üè¢ Compagnie a√©rienne

* **Reporting\_Airline** : Code transporteur (ex. "WN" pour Southwest Airlines)
* **DOT\_ID\_Reporting\_Airline** : Identifiant unique attribu√© par le Department of Transportation
* **IATA\_CODE\_Reporting\_Airline** : Code IATA (souvent identique √† `Reporting_Airline`)
* **Tail\_Number** : Immatriculation de l'avion
* **Flight\_Number\_Reporting\_Airline** : Num√©ro de vol tel que rapport√© par la compagnie

### üõ´ A√©roport de d√©part (origine)

* **OriginAirportID** : Identifiant unique de l‚Äôa√©roport d‚Äôorigine
* **OriginAirportSeqID** : Identifiant s√©quentiel pour l‚Äôa√©roport d‚Äôorigine
* **OriginCityMarketID** : Identifiant du march√© a√©rien de la ville d‚Äôorigine
* **Origin** : Code IATA de l‚Äôa√©roport (ex. "CMH" pour Columbus)
* **OriginCityName** : Nom complet de la ville d‚Äôorigine
* **OriginState** : Code de l‚Äô√âtat (ex. "OH")
* **OriginStateFips** : Code FIPS de l‚Äô√âtat
* **OriginStateName** : Nom complet de l‚Äô√âtat
* **OriginWac** : Code WAC (World Area Code)

### üõ¨ A√©roport de destination

* **DestAirportID** : Identifiant unique de l‚Äôa√©roport de destination
* **DestAirportSeqID** : Identifiant s√©quentiel de l‚Äôa√©roport de destination
* **DestCityMarketID** : Identifiant du march√© de la ville de destination
* **Dest** : Code IATA de destination
* **DestCityName** : Nom complet de la ville de destination
* **DestState** : Code √âtat destination
* **DestStateFips** : Code FIPS
* **DestStateName** : Nom complet de l‚Äô√âtat
* **DestWac** : Code WAC

### üï∞Ô∏è Horaires et retards

* **CRSDepTime** : Heure planifi√©e de d√©part (au format HHMM)

* **DepTime** : Heure r√©elle de d√©part

* **DepDelay** : D√©lai de d√©part (en minutes ; peut √™tre n√©gatif)

* **DepDelayMinutes** : D√©lai de d√©part (valeurs < 0 mises √† 0)

* **DepDel15** : 1 si le vol a eu ‚â•15 min de retard au d√©part, 0 sinon

* **DepartureDelayGroups** : Groupe de retard (cat√©gorie de -2 √† 12)

* **DepTimeBlk** : Plage horaire du d√©part (ex. "0600-0659")

* **TaxiOut** : Temps de roulage entre le d√©collage et la sortie du tarmac (en min)

* **WheelsOff** : Heure √† laquelle l'avion a quitt√© le sol

* **WheelsOn** : Heure √† laquelle l‚Äôavion a atterri

* **TaxiIn** : Temps de roulage apr√®s l'atterrissage jusqu'√† la porte

* **CRSArrTime** : Heure pr√©vue d‚Äôarriv√©e

* **ArrTime** : Heure r√©elle d‚Äôarriv√©e

* **ArrDelay** : D√©lai d‚Äôarriv√©e (en min, n√©gatif si en avance)

* **ArrDelayMinutes** : Retard d‚Äôarriv√©e (valeurs < 0 mises √† 0)

* **ArrDel15** : 1 si ‚â•15 min de retard √† l‚Äôarriv√©e, 0 sinon

* **ArrivalDelayGroups** : Groupe de retard √† l‚Äôarriv√©e (de -2 √† 12)

* **ArrTimeBlk** : Plage horaire d‚Äôarriv√©e pr√©vue

### üö´ Annulations et d√©routements

* **Cancelled** : 1 si vol annul√©, 0 sinon

* **CancellationCode** : Raison de l‚Äôannulation :

  * A = transporteur
  * B = m√©t√©o
  * C = syst√®me national de l‚Äôespace a√©rien (NAS)
  * D = s√©curit√©

* **Diverted** : 1 si le vol a √©t√© d√©rout√©, 0 sinon

### ‚è±Ô∏è Dur√©es

* **CRSElapsedTime** : Dur√©e planifi√©e du vol (en minutes)
* **ActualElapsedTime** : Dur√©e r√©elle
* **AirTime** : Temps de vol effectif (exclut taxiing)
* **Flights** : Nombre de vols (g√©n√©ralement 1)
* **Distance** : Distance en miles
* **DistanceGroup** : Groupe de distance (1 = 1-250 miles, ..., 11 = >2250 miles)

### üïµÔ∏è‚Äç‚ôÄÔ∏è Retards par cause (en min)

* **CarrierDelay** : Retard imputable √† la compagnie
* **WeatherDelay** : Retard caus√© par la m√©t√©o
* **NASDelay** : Retard d√ª au syst√®me a√©rien (trafic, contr√¥les, etc.)
* **SecurityDelay** : Retard li√© √† la s√©curit√©
* **LateAircraftDelay** : Retard caus√© par l‚Äôarriv√©e tardive d‚Äôun avion pr√©c√©dent

### üß≠ Diversion (d√©tail si le vol a √©t√© d√©rout√©)

* **DivAirportLandings** : Nombre d‚Äôatterrissages dans des a√©roports alternatifs
* **Div1Airport**, ..., **Div5Airport** : A√©roports de d√©routement (si applicable)
* **Div1WheelsOn**, ..., **Div5WheelsOn** : Heure d‚Äôatterrissage dans l‚Äôa√©roport d√©rout√©
* **Div1TotalGTime** : Temps total au sol lors du d√©routement
* **Div1TailNum** : Num√©ro d'immatriculation de l'appareil utilis√© dans la diversion

---


## üì• Exemple de t√©l√©chargement

Pour r√©cup√©rer les donn√©es d‚Äôun mois sp√©cifique depuis le site officiel du Bureau of Transportation Statistics (BTS), on peut utiliser les commandes suivantes en ligne de commande :

```bash
# D√©finition de l'URL de base pointant vers le r√©pertoire des fichiers ZIP
BTS=https://transtats.bts.gov/PREZIP
BASEURL="${BTS}/On_Time_Reporting_Carrier_On_Time_Performance_1987_present"

# On pr√©cise l'ann√©e et le mois souhait√©s (ici mars 2015)
YEAR=2015
MONTH=3

# T√©l√©chargement du fichier ZIP correspondant √† mars 2015
curl -k -o temp.zip ${BASEURL}_${YEAR}_${MONTH}.zip
```

* `curl -k -o temp.zip ...` : t√©l√©charge le fichier ZIP contenant les donn√©es brutes pour le mois sp√©cifi√©.

  * `-k` : ignore les erreurs de certificat SSL (utile si le site utilise un certificat auto-sign√©).
  * `-o temp.zip` : enregistre le fichier sous le nom `temp.zip`.

Ensuite, pour d√©compresser le fichier ZIP t√©l√©charg√© :

```bash
unzip temp.zip
```

Cela extrait un fichier `.csv` contenant les donn√©es des vols du mois s√©lectionn√©.

On peut ensuite afficher un aper√ßu des premi√®res lignes du fichier CSV avec :

```bash
head -5 *.csv
```

Cette commande permet de :

* rep√©rer le nom exact du fichier CSV,
* visualiser les 5 premi√®res lignes pour comprendre la structure des donn√©es (noms de colonnes + quelques exemples).

Ce processus est simple et efficace pour explorer les donn√©es mois par mois directement depuis le terminal.

---


## üì¶ Automatisation du t√©l√©chargement mensuel des donn√©es

### ‚ñ∂Ô∏è Exemple pour un mois donn√© : t√©l√©chargement de `201503.csv`

Pour r√©cup√©rer les donn√©es de performance des vols du mois de **mars 2015**, on peut automatiser le t√©l√©chargement avec une s√©rie de commandes bash simples :

```bash
YEAR=2015
MONTH=3
MONTH2=$(printf "%02d" $MONTH)  # Formatage du mois sur deux chiffres (ex: 03)

TMPDIR=$(mktemp -d)             # Cr√©ation d‚Äôun dossier temporaire
ZIPFILE=${TMPDIR}/${YEAR}_${MONTH2}.zip

# T√©l√©chargement du fichier ZIP depuis le site de la BTS
curl -o $ZIPFILE https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_${YEAR}_${MONTH}.zip

# D√©compression dans le dossier temporaire
unzip -d $TMPDIR $ZIPFILE

# D√©placement et renommage du fichier CSV en "201503.csv"
mv $TMPDIR/*.csv ./${YEAR}${MONTH2}.csv

# Suppression du dossier temporaire
rm -rf $TMPDIR
```

Ces commandes permettent :

* d'automatiser le t√©l√©chargement du fichier compress√© `.zip`,
* de l‚Äôextraire,
* de renommer proprement le fichier `.csv`,
* et de nettoyer les fichiers temporaires.

---

### üîÅ G√©n√©ralisation via le script `download.sh`

Afin d‚Äôautomatiser le t√©l√©chargement pour **plusieurs mois**, on a encapsul√© les √©tapes ci-dessus dans un script bash r√©utilisable : `download.sh`.

```bash
bash download.sh 2015 3
```

Cela ex√©cutera les m√™mes √©tapes que ci-dessus, mais pour n‚Äôimporte quelle **ann√©e** et **mois** donn√©s en param√®tres.

Voici un exemple d‚Äôautomatisation compl√®te pour **t√©l√©charger tous les mois de l‚Äôann√©e 2015** :

```bash
YEAR=2015
for MONTH in `seq 1 12`; do
   bash download.sh $YEAR $MONTH
done
```

Chaque appel √† `download.sh` t√©l√©charge le fichier `YYYYMM.csv` correspondant dans le r√©pertoire courant (par exemple : `201501.csv`, `201502.csv`, ‚Ä¶).

---

Ce syst√®me rend le processus **simple, propre et scalable** pour tous les mois et toutes les ann√©es disponibles dans la base de donn√©es du [Bureau of Transportation Statistics](https://transtats.bts.gov/OT_Delay/OT_DelayCause1.asp?).

---

## üìä Analyse initiale des fichiers CSV (2015)

Une fois les fichiers `2015MM.csv` t√©l√©charg√©s et extraits pour chaque mois, il est utile de faire une premi√®re analyse en ligne de commande pour **√©valuer la structure et la taille** des donn√©es.

### üßÆ Nombre de colonnes

On peut compter rapidement le **nombre de colonnes** du fichier en regardant une ligne d‚Äôexemple (ici la premi√®re ligne de donn√©es) et en comptant les s√©parateurs :

```bash
head -2 201503.csv | tail -1 | sed 's/,/ /g' | wc -w
```

Ce qui donne :

```bash
81
```

üëâ Le fichier contient donc **81 colonnes**, ce qui refl√®te la richesse des informations disponibles pour chaque vol.

### üìà Nombre de lignes par mois

On peut aussi compter le **nombre de lignes (vols)** dans chaque fichier CSV correspondant √† un mois de l‚Äôann√©e 2015 :

```bash
wc -l *.csv
```

Sortie typique :

```
    469969 201501.csv
    429192 201502.csv
    504313 201503.csv
    485152 201504.csv
    496994 201505.csv
    503898 201506.csv
    520719 201507.csv
    510537 201508.csv
    464947 201509.csv
    486166 201510.csv
    467973 201511.csv
    479231 201512.csv
   6323404 total
```

üëâ Cela repr√©sente **plus de 6 millions de vols** enregistr√©s en 2015 !

### ‚ö†Ô∏è Pourquoi c‚Äôest important ?

* Ces commandes permettent d‚Äôavoir une id√©e de la **taille des donn√©es √† manipuler**.
* La lenteur de ces commandes en ligne de commande donne un indice sur le fait que **toute analyse ‚Äúfull scan‚Äù risque d‚Äô√™tre co√ªteuse en temps**, surtout en local.
* On comprend d√®s lors l‚Äôint√©r√™t d‚Äôutiliser des outils comme **BigQuery** ou **Spark** pour une mont√©e en √©chelle efficace.

üí° *Ce genre d'analyse exploratoire simple montre √† quel point quelques comp√©tences en scripting Unix (bash, wc, sed, etc.) peuvent √™tre pr√©cieuses au d√©but d‚Äôun projet Data.*

---

## ‚òÅÔ∏è T√©l√©chargement de donn√©es vers Google Cloud Storage

### üîé Qu‚Äôest-ce que Google Cloud Storage (GCS) ?

**Google Cloud Storage (GCS)** est un service de stockage d‚Äôobjets dans le cloud propos√© par Google Cloud Platform. Il permet de stocker tout type de fichier (CSV, images, vid√©os, mod√®les, backups, etc.) dans des *buckets* (conteneurs), un peu comme des dossiers, avec une grande durabilit√© et accessibilit√© mondiale.

Dans notre projet, nous allons utiliser GCS pour stocker les fichiers de vols (.csv) avant de les exploiter avec d‚Äôautres services comme BigQuery ou Dataflow.

---

### üöÄ √âtapes de transfert des fichiers vers GCS

Voici les commandes utilis√©es pour cr√©er un bucket GCS et y envoyer les fichiers CSV :

```bash
# R√©cup√©ration du projet actif configur√© avec gcloud
PROJECT=$(gcloud config get-value project)

# Nom du bucket = nom du projet + suffixe explicite
BUCKET=${PROJECT}-dsongcp

# Sp√©cification de la r√©gion
REGION=us-central1

# Cr√©ation du bucket de stockage dans la r√©gion sp√©cifi√©e
gcloud storage buckets create gs://$BUCKET --location=$REGION

# Envoi des fichiers CSV dans un dossier 'flights/raw' du bucket
gcloud storage cp *.csv gs://$BUCKET/flights/raw
```

#### üß† Explications :

* `gcloud config get-value project` : r√©cup√®re l'ID du projet GCP actif.
* `gcloud storage buckets create` : cr√©e un nouveau bucket de stockage.
* `gcloud storage cp` : copie tous les fichiers CSV vers le bucket dans un dossier `flights/raw`.

üìÅ Au final, vos donn√©es se retrouveront dans ce chemin cloud :

```
gs://<votre-projet>-dsongcp/flights/raw/
```

Cela constitue une **√©tape cl√© de l‚Äôingestion de donn√©es** dans le cloud : les fichiers sont pr√™ts √† √™tre utilis√©s par d'autres services Google Cloud (ex. : Dataflow, BigQuery, Dataproc).

---

## üß≠ Chargement des donn√©es dans BigQuery

### üîç Qu‚Äôest-ce que BigQuery ?

[**BigQuery**](https://cloud.google.com/bigquery) est un *entrep√¥t de donn√©es* (Data Warehouse) serverless propos√© par Google Cloud. Il permet de stocker et d'interroger de tr√®s grands volumes de donn√©es avec une latence tr√®s faible gr√¢ce √† une architecture massivement parall√®le.

BigQuery est parfaitement adapt√© pour des cas d‚Äôanalyse de donn√©es √† grande √©chelle comme :

* l‚Äôexploration de donn√©es brutes,
* la cr√©ation de tableaux de bord,
* l‚Äôentra√Ænement de mod√®les de Machine Learning,
* ou encore l‚Äôalimentation d'applications m√©tiers.

---

### üìä ELT vs ETL : un changement de paradigme

La pratique moderne privil√©gie de plus en plus l‚Äôapproche **ELT (Extract - Load - Transform)** √† l‚Äôancienne m√©thode **ETL (Extract - Transform - Load)**.
üëâ Dans ELT :

* les donn√©es sont **charg√©es brutes** dans le Data Warehouse,
* puis **transform√©es √† la demande** via des requ√™tes SQL ou des outils sp√©cialis√©s (dbt, Dataform, etc.).

Cela permet plus de flexibilit√©, de tra√ßabilit√© et d‚Äôagilit√©, comme illustr√© ci-dessous :

![ELT diagram](./5a957722-3bd1-4588-b782-8d6ffd853843.png)

---

### üöÄ Chargement d‚Äôun fichier CSV dans BigQuery

Voici les commandes de base utilis√©es pour cr√©er un dataset et charger un fichier CSV brut dans une table :

```bash
# Cr√©ation d‚Äôun dataset BigQuery nomm√© 'dsongcp' dans la r√©gion sp√©cifi√©e
bq --location=$REGION mk dsongcp

# Chargement automatique d‚Äôun fichier CSV dans une table 'flights_auto'
bq load --autodetect --source_format=CSV dsongcp.flights_auto gs://${BUCKET}/flights/raw/201501.csv
```

‚úÖ Cette commande utilise l‚Äôoption `--autodetect` pour inf√©rer automatiquement le sch√©ma. C‚Äôest pratique au d√©but, mais pas recommand√© en production car elle peut conduire √† des erreurs ou des types incorrects.

Via la [Console Web de BigQuery](https://console.cloud.google.com/bigquery), tu peux tester une requ√™te sur la table :

```sql
SELECT
    ORIGIN,
    AVG(DEPDELAY) AS dep_delay,
    AVG(ARRDELAY) AS arr_delay,
    COUNT(ARRDELAY) AS num_flights
FROM
    dsongcp.flights_auto
GROUP BY
    ORIGIN
ORDER BY num_flights DESC
LIMIT 10;
```

---

### üìâ Pourquoi partitionner les tables dans BigQuery ?

BigQuery facture √† la requ√™te selon le volume de donn√©es scann√©es. Partitionner une table, par exemple par **mois de vol (`FlightDate`)**, permet de :

* r√©duire consid√©rablement le **co√ªt des requ√™tes**,
* am√©liorer les **performances**,
* simplifier la **gestion temporelle des donn√©es**.

---

### üßπ Rechargement avec un sch√©ma explicite et partitionnement mensuel

Nous allons maintenant supprimer la table automatique et cr√©er des tables partitionn√©es par mois avec un sch√©ma bien d√©fini :

```bash
bq rm -f -t dsongcp.flights_auto
touch bqload.sh
```

---

### üìú Script `bqload.sh` ‚Äî Chargement robuste et scalable

Ce script charge les 12 fichiers mensuels de l‚Äôann√©e sp√©cifi√©e dans une table partitionn√©e `flights_raw`, avec un sch√©ma pr√©cis :

```bash
#!/bin/bash

if [ "$#" -ne 2 ]; then
    echo "Usage: ./bqload.sh csv-bucket-name YEAR"
    exit
fi

BUCKET=$1
YEAR=$2
SCHEMA=... # sch√©ma complet omis ici pour lisibilit√©

PROJECT=$(gcloud config get-value project)
bq --project_id $PROJECT show dsongcp || bq mk --sync dsongcp

for MONTH in `seq -w 1 12`; do
    CSVFILE=gs://${BUCKET}/flights/raw/${YEAR}${MONTH}.csv
    bq --project_id $PROJECT --sync \
       load --time_partitioning_field=FlightDate --time_partitioning_type=MONTH \
       --source_format=CSV --ignore_unknown_values --skip_leading_rows=1 --schema=$SCHEMA \
       --replace ${PROJECT}:dsongcp.flights_raw\$${YEAR}${MONTH} $CSVFILE
done
```

### üîç Ce que fait ce script :

| √âtape              | Description                                               |
| ------------------ | --------------------------------------------------------- |
| üéØ Param√®tres      | Prend le nom du bucket et l'ann√©e √† charger               |
| üìÇ Dataset         | Cr√©e le dataset `dsongcp` s‚Äôil n‚Äôexiste pas               |
| üìÅ Fichiers        | Parcourt les 12 mois et g√©n√®re le chemin des fichiers CSV |
| üß† Sch√©ma          | Utilise un sch√©ma explicite avec tous les champs de vol   |
| üìÜ Partitionnement | Utilise `FlightDate` pour cr√©er des partitions mensuelles |
| ‚ôªÔ∏è Remplacement    | Remplace les partitions si elles existent d√©j√†            |

---

üí° Une fois ce script ex√©cut√©, vous aurez une table **`dsongcp.flights_raw`** partitionn√©e mensuellement, pr√™te pour des requ√™tes optimis√©es dans BigQuery !

Pour ex√©cuter correctement le script `bqload.sh`, suis ces **3 √©tapes** :

---

### ‚úÖ 1. Donner les droits d‚Äôex√©cution

Avant de pouvoir lancer le script, rends-le ex√©cutable :

```bash
chmod +x bqload.sh
```

---

### üöÄ 2. Lancer le script avec les bons arguments

Le script attend deux arguments :

1. le **nom du bucket GCS** o√π se trouvent les fichiers CSV (ex. : `my-bucket-name`)
2. l‚Äô**ann√©e √† charger** (ex. : `2015`)

Exemple de commande :

```bash
./bqload.sh my-bucket-name 2015
```

---

### üìÇ 3. V√©rifier dans BigQuery

Une fois le script termin√© :

* Une table partitionn√©e `flights_raw` est cr√©√©e dans le dataset `dsongcp`
* Tu peux la visualiser avec :

```bash
bq ls dsongcp
```

Et tu peux tester une requ√™te simple sur une partition sp√©cifique :

```sql
SELECT * FROM dsongcp.flights_raw
WHERE FlightDate BETWEEN '2015-01-01' AND '2015-01-31'
LIMIT 10;
```

---

## Planification des t√©l√©chargements mensuels

### üß† Probl√®me de d√©part

Tu as d√©j√† t√©l√©charg√© les donn√©es historiques de vols (par exemple pour l‚Äôann√©e 2015) et tu les as mises dans un bucket Google Cloud Storage (GCS).
Mais ces donn√©es sont **actualis√©es chaque mois** par le site officiel du gouvernement am√©ricain (BTS). Tu veux donc une m√©thode **automatique** pour t√©l√©charger chaque mois les nouvelles donn√©es de vol.

---

### üéØ Objectif

Mettre en place un syst√®me **automatis√©**, **sans serveur** (pas besoin de g√©rer un ordinateur ou un serveur toi-m√™me) pour que, chaque mois :

1. Un script aille chercher les nouvelles donn√©es du site web du gouvernement.
2. Ces donn√©es soient t√©l√©charg√©es.
3. Puis envoy√©es automatiquement dans ton bucket Cloud Storage.

---

### üõ†Ô∏è Outils utilis√©s

| Outil Google Cloud  | R√¥le dans le processus                                                                                                               |
| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| **Cloud Scheduler** | C‚Äôest comme un `cron` sur Linux : il permet de lancer une t√¢che automatiquement selon un calendrier (ex : le 3 de chaque mois √† 7h). |
| **Cloud Run**       | Permet d‚Äôex√©cuter ton code dans un conteneur Docker **via une simple URL**, sans g√©rer de serveur.                                   |
| **Flask (Python)**  | Framework l√©ger qui transforme ton code Python en **service web** (accessible par une URL).                                          |
| **Docker**          | Sert √† emballer ton application Python dans un "conteneur" pr√™t √† √™tre ex√©cut√© par Cloud Run.                                        |

---

### üì¶ Strat√©gie compl√®te pas √† pas

1. **Tu cr√©es un script Python `ingest_flights.py`**
   ‚ûú Ce script est capable de t√©l√©charger les donn√©es pour un **mois donn√©** (ex: janvier 2024), et les envoyer vers Cloud Storage.

2. **Tu encapsules ce script dans une application Flask (`main.py`)**
   ‚ûú Gr√¢ce √† Flask, tu peux appeler ton script via une **URL** (ex: `https://.../ingest?year=2024&month=01`).

3. **Tu emballes tout √ßa dans un conteneur Docker**
   ‚ûú Le conteneur contient tout ce qu‚Äôil faut pour ex√©cuter ton code.

4. **Tu d√©ploies le conteneur sur Cloud Run**
   ‚ûú Tu obtiens une URL unique pour appeler ton application Flask.

5. **Tu cr√©es une t√¢che dans Cloud Scheduler**
   ‚ûú Elle est configur√©e pour **appeler cette URL tous les mois** (ex : le 5 de chaque mois √† 3h du matin).

---

### üí¨ En r√©sum√© (image mentale simple)

Imagine un **robot invisible** (Cloud Scheduler) qui, tous les mois, va visiter un site web (ton app Flask sur Cloud Run).
Quand il arrive sur la page `https://.../ingest?year=2024&month=07`, ton script Python `ingest_flights.py` se d√©clenche automatiquement.
Il t√©l√©charge les nouvelles donn√©es du mois de juillet et les range proprement dans ton bucket Google Cloud Storage.

---

## üß† R√©sum√© de tout le fichier `ingest_flights.py`

Le fichier complet `ingest_flights.py` constitue un **mini pipeline ETL** depuis les donn√©es publiques du gouvernement am√©ricain jusqu‚Äô√† **BigQuery**, en passant par Google Cloud Storage (GCS).

### 1. üì• T√©l√©charger les donn√©es ZIP depuis le site de la BTS (Bureau of Transportation Statistics)

```python
zipfile = download(year, month, tempdir)
```

üìå La fonction `download()` utilise une URL bas√©e sur la variable `SOURCE` (`SOURCE = "https://transtats.bts.gov/PREZIP"`) pour t√©l√©charger le ZIP du mois/ann√©e demand√©.

---

### 2. üßæ Extraire et compresser le CSV

```python
bts_csv = zip_to_csv(zipfile, tempdir)
```

üìå Extrait le fichier `.csv` depuis l‚Äôarchive `.zip` puis le compresse en `.csv.gz`.

---

### 3. ‚òÅÔ∏è Uploader sur GCS

```python
gcsloc = upload(bts_csv, bucket, gcsloc)
```

üìå Upload du fichier `.csv.gz` dans un bucket Google Cloud Storage (`gs://...`).

---

### 4. üß© Charger dans BigQuery (avec partition mensuelle)

```python
return bqload(gcsloc, year, month)
```

üìå Charge le fichier GCS dans une table BigQuery `dsongcp.flights_raw`, **partitionn√©e par mois via la colonne `FlightDate`**.

La table cible est du type :

```text
flights_raw$201501
```

> ‚úÖ Il s‚Äôagit d‚Äôun **chargement partitionn√©** qui **remplace** les donn√©es si elles existent d√©j√† (`WRITE_TRUNCATE`).

---

### 5. üîÅ Fonction `ingest(year, month, bucket)`

**C‚Äôest le point d‚Äôentr√©e unique de tout le fichier**.

Elle orchestre :

1. Le t√©l√©chargement
2. L‚Äôextraction + compression
3. L‚Äôupload vers GCS
4. Le chargement dans BigQuery

> Cette fonction est id√©ale pour √™tre appel√©e depuis un script ou une API (comme un endpoint Flask ou FastAPI), par exemple :

```python
ingest("2015", "01", "mon-bucket-gcs")
```

---

### 6. üìÜ `next_month(bucketname)`

Fonction utile pour automatiser le traitement mensuel.

* Elle va scanner le bucket GCS (`flights/raw/*.csv.gz`) pour voir le dernier fichier.
* Elle extrait le `year` et `month` du dernier fichier trouv√©.
* Puis elle appelle `compute_next_month()` pour calculer le mois suivant.

Exemple :

* Si le dernier fichier est `flights/raw/202401.csv.gz`, il retournera `2024,02`.

---

### 7. üóìÔ∏è `compute_next_month(year, month)`

Ajoute 30 jours √† une date fixe du mois courant (`15`), ce qui garantit de passer au mois suivant sans edge case sur la longueur du mois.

---

### 8. üñ•Ô∏è Mode CLI (Command-Line Interface)

Bloc ex√©cutable depuis un terminal :

```bash
python3 ingest_flights.py --bucket $BUCKET --year 2023 --month 05
```

Ou si on veut laisser le script choisir le **mois suivant automatiquement** :

```bash
python3 ingest_flights.py --bucket $BUCKET
```

Avec logging en mode debug :

```bash
python3 ingest_flights.py --bucket $BUCKET --debug
```

---

## Ex√©cution test du script `ingest_flights.py`

```bash
python3 ingest_flights.py --bucket $BUCKET
```

### Erreur attendue si les identifiants ne sont pas configur√©s

Si vos identifiants par d√©faut ne sont pas encore configur√©s, vous obtiendrez une erreur similaire √† :

```bash
 File "/home/vant/Documents/learning/datascience-on-gcp/.venv/lib/python3.12/site-packages/google/auth/_default.py", line 685, in default
    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
```

üëâ **Explication de l'erreur :**
Cette erreur indique que le script n‚Äôa pas trouv√© d'identifiants valides pour s‚Äôauthentifier aupr√®s des services Google Cloud. Il faut donc configurer un compte de service et fournir sa cl√© au script.

---

### Cr√©ation d'un compte de service en tant qu'identit√© de notre script

Un **compte de service** est une identit√© utilis√©e par des applications ou des services automatis√©s pour interagir avec Google Cloud avec des permissions pr√©cises et limit√©es. Voici les √©tapes pour en cr√©er un et lui attribuer les droits n√©cessaires :

```bash
SVC_ACCT=svc-monthly-ingest                     # Nom du compte de service
PROJECT=$(gcloud config get-value project)      # R√©cup√©ration du projet actif
REGION=us-central1                              # R√©gion (√† adapter si besoin)
BUCKET=${PROJECT}-dsongcp                       # Nom du bucket
SVC_PRINCIPAL=serviceAccount:${SVC_ACCT}@${PROJECT}.iam.gserviceaccount.com
```

#### Cr√©ation du compte de service

```bash
gcloud iam service-accounts create $SVC_ACCT --display-name "flights monthly ingest"
```

‚û° Cr√©e un compte de service nomm√© `svc-monthly-ingest` avec une description.

---

#### Activation de l'acc√®s uniforme au bucket

```bash
gsutil uniformbucketlevelaccess set on gs://$BUCKET
```

‚û° Active la gestion uniforme des permissions au niveau du bucket (et non des objets).

---

#### Attribution des droits sur le bucket

```bash
gsutil iam ch ${SVC_PRINCIPAL}:roles/storage.admin gs://$BUCKET
```

‚û° Donne au compte de service le r√¥le **Storage Admin** sur le bucket pour g√©rer les fichiers.

---

#### Attribution des droits BigQuery

```bash
bq --project_id=${PROJECT} query --nouse_legacy_sql \
  "GRANT \`roles/bigquery.dataOwner\` ON SCHEMA dsongcp TO '$SVC_PRINCIPAL'"
```

‚û° Donne au compte de service le r√¥le **dataOwner** sur le dataset BigQuery `dsongcp`.

```bash
gcloud projects add-iam-policy-binding ${PROJECT} \
  --member ${SVC_PRINCIPAL} \
  --role roles/bigquery.jobUser
```

‚û° Permet au compte de service de lancer des jobs BigQuery.

---

#### Cr√©ation de la cl√© du compte de service

```bash
gcloud iam service-accounts keys create key.json \
  --iam-account=${SVC_ACCT}@${PROJECT}.iam.gserviceaccount.com
```

‚û° G√©n√©re un fichier `key.json` contenant la cl√© priv√©e du compte de service. Ce fichier sera utilis√© par votre script pour s‚Äôauthentifier.

---

#### D√©finition des identifiants applicatifs

```bash
export GOOGLE_APPLICATION_CREDENTIALS=key.json
```

‚û° Indique au SDK Google Cloud et aux biblioth√®ques clientes d‚Äôutiliser cette cl√© pour s‚Äôauthentifier.

---

### Ex√©cution du pipeline

Apr√®s avoir configur√© les identifiants et les permissions, nous avons lanc√© plusieurs ex√©cutions du script pour ing√©rer les donn√©es des vols mois par mois. Voici un exemple des commandes et des r√©sultats observ√©s :

```bash
python3 ingest_flights.py --bucket $BUCKET
INFO: Requesting data for 2016-01-*
INFO: Extracted /tmp/ingest_flightsrvokv_6v/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2016_1.csv
INFO: Compressed into /tmp/ingest_flightsrvokv_6v/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2016_1.csv.gz
INFO: <Bucket: ds-on-gcp-464823-dsongcp>
INFO: Uploaded gs://ds-on-gcp-464823-dsongcp/flights/raw/201601.csv.gz ...
INFO: Success ... ingested 445827 rows to ds-on-gcp-464823.dsongcp.flights_raw$201601
```

‚û° Le script t√©l√©charge les donn√©es de janvier 2016, les compresse, les charge sur le bucket GCS, puis les importe dans BigQuery (445 827 lignes).

---

```bash
python3 ingest_flights.py --bucket $BUCKET
INFO: Requesting data for 2016-02-*
...
INFO: Success ... ingested 423889 rows to ds-on-gcp-464823.dsongcp.flights_raw$201602
```

‚û° F√©vrier 2016 : 423 889 lignes ing√©r√©es.

---

```bash
python3 ingest_flights.py --bucket $BUCKET
INFO: Requesting data for 2016-03-*
...
INFO: Success ... ingested 451236 rows to ds-on-gcp-464823.dsongcp.flights_raw$201603
```

‚û° Mars 2016 : 451 236 lignes ing√©r√©es.

---

```bash
python3 ingest_flights.py --bucket $BUCKET
INFO: Requesting data for 2016-04-*
...
INFO: Success ... ingested 461630 rows to ds-on-gcp-464823.dsongcp.flights_raw$201604
```

‚û° Avril 2016 : 461 630 lignes ing√©r√©es.

---

### V√©rification des donn√©es dans BigQuery

Pour v√©rifier que les donn√©es ont bien √©t√© import√©es dans BigQuery, vous pouvez ex√©cuter une requ√™te comme celle-ci dans la console BigQuery ou en ligne de commande :

```sql
SELECT * 
FROM dsongcp.flights_raw
WHERE FlightDate BETWEEN '2016-04-01' AND '2016-04-30';
```

‚úÖ **R√©sultat attendu :**
La requ√™te retourne bien **461 630 lignes**, correspondant aux donn√©es du mois d‚Äôavril 2016.

---

Si tu veux, je peux aussi ajouter un paragraphe pour expliquer comment automatiser ce pipeline (par exemple via Cloud Scheduler ou un DAG Airflow), ou proposer un sch√©ma de l‚Äôarchitecture‚ÄØ! Dis-le-moi‚ÄØ!



